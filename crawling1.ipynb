{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5주차 데이터 수집 (크롤링)\n",
    "\n",
    "데이터를 수집하는 방법은 여러가지가 있습니다. csv(comma seperated values)나 엑셀과 같이 이미 정리된 [파일 형식으로 데이터](http://datalab.naver.com/)를 얻을 수도 있고, Database나 [API](https://developers.naver.com/products/login/api)를 통해 얻을수도 있지만, 오늘은 **웹 크롤링**을 통해 데이터를 수집하는 방법에 대해서 배워보겠습니다. Web crawling은 Web scraping라고도 말하는데, 위키피디아에 따르면 **'조직적이고 자동화된 방법으로 월드 와이드 웹을 탐색하는 방법’**이라고 나와있네요. HTML 파일을 긁어서 필요한 정보들만 뽑아 csv 파일로 저장하는 방법에 대해 배워보겠습니다.\n",
    "\n",
    "웹사이트 크롤링을 하기 위해선 **우선 웹사이트를 구성하고 있는 HTML, CSS**에 대한 기본적인 지식이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML과 CSS에 대한 기본 지식\n",
    "- [W3School](http://www.w3schools.com/html/)\n",
    "- [codeacademy](https://www.codecademy.com/learn/web)\n",
    "\n",
    "### 1. 파이참에서 index.html파일을 만들어봅시다.\n",
    "### 2. 바꿔봅시다!\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>파이썬 클래스</title>\n",
    "    <meta property=\"og:title\" content=\"페이스북이나 카톡 미리보기에서 보이는 제목입니다.\" />\n",
    "    <meta property=\"og:description\" content=\"og 설명\" />\n",
    "    <meta property=\"og:image\" content=\"og 이미지\" />\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #d0e4fe;\n",
    "        }\n",
    "\n",
    "        h1 {\n",
    "            color: orange;\n",
    "            text-align: center;\n",
    "        }\n",
    "\n",
    "        .desc-box {\n",
    "            text-align: center;\n",
    "        }\n",
    "\n",
    "        #desc-important {\n",
    "            color: blue;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>파이썬 2016 클래스입니다.</h1>\n",
    "\n",
    "<div class=\"desc-box\">\n",
    "    <p>오늘은 <b id=\"crawling\">크롤링</b>에 대해 배우고 있습니다.</p>\n",
    "\n",
    "    <div>\n",
    "        <img src=\"http://movement-as-medicine.com/wp-content/uploads/2014/01/baby-crawling.jpg\"/>\n",
    "    </div>\n",
    "\n",
    "    <span>크롤링에 대한 자세한 정의는 <a href=\"https://en.wikipedia.org/wiki/Web_crawler\">위키피디아</a>를 참고해보세요.</span>\n",
    "\n",
    "    <div class=\"desc-line first\">\n",
    "        <span>웹 크롤링을 하려면 html과 css에 대해 먼저 알아야 합니다.</span>\n",
    "        <a href=\"http://www.w3schools.com/html/\">w3school 페이지에서 HTML, CSS에 대해 알아볼 수 있어요</a>\n",
    "    </div>\n",
    "    <div class=\"desc-line second\">\n",
    "        <span><b id=\"desc-important\">Javascript</b>는 웹페이지를 동적으로 만들어주는 역할을 합니다.</span>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "1. 마우스 오른쪽 버튼 -> 검사\n",
    "2. 마우스 오른쪽 버튼 -> Copy -> CSS Selector\n",
    "3. body > div > div.desc-line.second\n",
    "\n",
    "### 3. style 태그 대신 style.css 파일에 정리해둘 수도 있습니다.\n",
    "`<head>`에 추가해주세요.\n",
    "\n",
    "```html\n",
    "<link rel=\"stylesheet\" href=\"style.css\">\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오늘 수업에 필요한 패키지를 설치해주세요\n",
    "```\n",
    "> pip install beautifulsoup4\n",
    "> pip install requests\n",
    "> pip install urllib3\n",
    "```\n",
    "\n",
    "개발 도중 사용법에 대해 궁금한 것들이 있으면 Document를 참고해보세요.\n",
    "- [beautifulsoup4](http://coreapython.hosting.paran.com/etc/beautifulsoup4.html): HTML로 부터 데이터를 뽑아내기 위한 라이브러리입니다.\n",
    "- [requests](http://docs.python-requests.org/en/master/): requests.get을 통해 HTML을 받아올 수 있습니다.\n",
    "- [urllib3](https://urllib3.readthedocs.org/en/latest/): HTTP 통신을 도와주는 패키지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 뷰티플수프는 HTML에서 원하는 데이터를 쉽게 뽑아낼 수 있는 파이썬 라이브러리입니다.\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 만든 index.html부터 살펴봅시다.\n",
    "soup = bs(open('./index.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>파이썬 클래스</title>\n",
      "title\n",
      "파이썬 클래스\n",
      "head\n",
      "<p>오늘은 <b id=\"crawling\">크롤링</b>에 대해 배우고 있습니다.</p>\n"
     ]
    }
   ],
   "source": [
    "# soup 객체로부터 쉽게 parsing된 데이터들을 받아올 수 있습니다.\n",
    "\n",
    "# title 태그를 받아옵니다.\n",
    "print soup.title\n",
    "\n",
    "# 태그 이름을 받아옵니다.\n",
    "print soup.title.name\n",
    "\n",
    "# 태그 내 문자열을 가져옵니다.\n",
    "print soup.title.text\n",
    "\n",
    "# 부모 태그를 가져옵니다.\n",
    "print soup.title.parent.name\n",
    "\n",
    "# p 태그를 가져옵니다.\n",
    "print soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://en.wikipedia.org/wiki/Web_crawler\">위키피디아</a>, <a href=\"http://www.w3schools.com/html/\">w3school 페이지에서 HTML, CSS에 대해 알아볼 수 있어요</a>]\n"
     ]
    }
   ],
   "source": [
    "# 모든 a 태그 가져오기\n",
    "print soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"desc-box\">\n",
      "<p>오늘은 <b id=\"crawling\">크롤링</b>에 대해 배우고 있습니다.</p>\n",
      "<div>\n",
      "<img src=\"http://movement-as-medicine.com/wp-content/uploads/2014/01/baby-crawling.jpg\"/>\n",
      "</div>\n",
      "<span>크롤링에 대한 자세한 정의는 <a href=\"https://en.wikipedia.org/wiki/Web_crawler\">위키피디아</a>를 참고해보세요.</span>\n",
      "<div class=\"desc-line first\">\n",
      "<span>웹 크롤링을 하려면 html과 css에 대해 먼저 알아야 합니다.</span>\n",
      "<a href=\"http://www.w3schools.com/html/\">w3school 페이지에서 HTML, CSS에 대해 알아볼 수 있어요</a>\n",
      "</div>\n",
      "<div class=\"desc-line second\">\n",
      "<span><b id=\"desc-important\">Javascript</b>는 웹페이지를 동적으로 만들어주는 역할을 합니다.</span>\n",
      "</div>\n",
      "</div>]\n",
      "[<b id=\"desc-important\">Javascript</b>]\n",
      "[<span>웹 크롤링을 하려면 html과 css에 대해 먼저 알아야 합니다.</span>]\n",
      "[<a href=\"http://www.w3schools.com/html/\">w3school 페이지에서 HTML, CSS에 대해 알아볼 수 있어요</a>]\n",
      "[<div class=\"desc-line first\">\n",
      "<span>웹 크롤링을 하려면 html과 css에 대해 먼저 알아야 합니다.</span>\n",
      "<a href=\"http://www.w3schools.com/html/\">w3school 페이지에서 HTML, CSS에 대해 알아볼 수 있어요</a>\n",
      "</div>, <div class=\"desc-line second\">\n",
      "<span><b id=\"desc-important\">Javascript</b>는 웹페이지를 동적으로 만들어주는 역할을 합니다.</span>\n",
      "</div>]\n"
     ]
    }
   ],
   "source": [
    "# 클래스명으로 가져오기\n",
    "print soup.select('.desc-box')\n",
    "\n",
    "# id로 가져오기\n",
    "print soup.select('#desc-important')\n",
    "\n",
    "# CSS path로 가져오기\n",
    "print soup.select('body > div > div.desc-line.first > span')\n",
    "\n",
    "# 속성(attr)으로 가져오기\n",
    "print soup.findAll('a', {'href': 'http://www.w3schools.com/html/'})\n",
    "\n",
    "# 클래스명도 속성 중 하나이기 때문에 이런식으로 가져올 수 있습니다.\n",
    "print soup.findAll('div', {'class': 'desc-line'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Javascript는 웹페이지를 동적으로 만들어주는 역할을 합니다.\n",
      "\n",
      "{'href': 'https://en.wikipedia.org/wiki/Web_crawler'}\n",
      "https://en.wikipedia.org/wiki/Web_crawler\n",
      "{'src': 'http://movement-as-medicine.com/wp-content/uploads/2014/01/baby-crawling.jpg'}\n",
      "http://movement-as-medicine.com/wp-content/uploads/2014/01/baby-crawling.jpg\n"
     ]
    }
   ],
   "source": [
    "# 자, 이제 필요한 HTML 태그를 뽑는 것 까지 가능합니다. \n",
    "# 태그 안에 속성(src, href 등)이나 text를 뽑아내는 방법에 대해 알아봅시다. \n",
    "\n",
    "# text만 뽑아내기\n",
    "second_line = soup.select('.desc-line.second')\n",
    "print second_line[0].text\n",
    "\n",
    "# a 태그 href 뽑아내기\n",
    "a_tag = soup.select('body > div > span > a')\n",
    "print a_tag[0].attrs\n",
    "print a_tag[0]['href']\n",
    "\n",
    "# img 태그 src 뽑아내기\n",
    "image_tag = soup.select('img')\n",
    "print image_tag[0].attrs\n",
    "print image_tag[0]['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 **beautifulsoup 기본기**는 닦은 것 같습니다. **requests** 라이브러리를 사용해 월드 와이드 웹 세상의 html을 받아옵시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip를 통해 설치한 requests를 import합니다.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 무료 이미지들을 크롤링해봅시다.\n",
    "res = requests.get('https://pixabay.com/en/')\n",
    "\n",
    "# dir()은 해당 객체의 사용 가능한 속성(attributes)를 list로 리턴해줍니다.\n",
    "# print dir(res) \n",
    "\n",
    "# HTTP 요청을 보내면 응답으로 Status Code가 오게 됩니다.\n",
    "# 200: 성공\n",
    "# 404: 페이지 없음\n",
    "# 500: 서버 오류\n",
    "# => https://ko.wikipedia.org/wiki/HTTP_%EC%83%81%ED%83%9C_%EC%BD%94%EB%93%9C\n",
    "\n",
    "# HTTP란 WWW에서 사용하는 약속(protocol)입니다. \n",
    "# 여러분들의 컴퓨터(클라이언트)와 네이버의 컴퓨터(서버)가 통신을 하는데 있어, \n",
    "# '어떤 메시지 형식으로 주고 받을 것이다.'하고 약속을 하고 0101..의 이진수를 주고 받는 것 이지요.\n",
    "# 이러한 약속을 미리 공유하고 있어야 메시지를 올바르게 해석할 수 있습니다.\n",
    "\n",
    "# 200번이면 성공적으로 잘 받아온겁니다.\n",
    "print res.status_code\n",
    "# print res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs(open('./index.html')) 했던 것 처럼, index.html 자리에 res.text를 넣어줍니다.\n",
    "# python에서 open은 파일을 읽거나 쓸때 사용됩니다.\n",
    "# 여기서 open 또한 index.html '파일'을 읽어오기 위해 사용된 것입니다.\n",
    "# res.text는 그 자체가 html이므로 open을 사용할 필요가 없습니다.\n",
    "pixabay_html = bs(res.text)\n",
    "\n",
    "# 메인 페이지의 이미지를 받아서, 로컬 컴퓨터에 저장해보세요.\n",
    "# 이미지를 로컬 컴퓨터에 다운받으려면 urllib 라이브러리를 사용하면 됩니다.\n",
    "# pip install urllib3\n",
    "import urllib\n",
    "def download_image(img_src, filename):\n",
    "    res = urllib.urlretrieve(img_src, filename)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': '/static/uploads/photo/2015/11/23/11/57/hot-chocolate-1058197__340.jpg', 'alt': '', 'srcset': '/static/uploads/photo/2015/11/23/11/57/hot-chocolate-1058197__340.jpg 1x, /static/uploads/photo/2015/11/23/11/57/hot-chocolate-1058197_640.jpg 2x'}\n",
      "https://pixabay.com/static/uploads/photo/2015/11/23/11/57/hot-chocolate-1058197__340.jpg hot-chocolate-1058197__340.jpg\n"
     ]
    }
   ],
   "source": [
    "# pixabay.com에서 이미지 크롤링해오기\n",
    "def crawl_image():\n",
    "    res = requests.get('https://pixabay.com/en/')\n",
    "    \n",
    "    if res.status_code == 200:\n",
    "        soup = bs(res.text)\n",
    "        images = soup.select('#gallery > div.flex_grid.credits > div > a > img')\n",
    "        \n",
    "        for image in images:\n",
    "            # 속성 출력하기\n",
    "            print image.attrs\n",
    "            \n",
    "            # 속성 중 이미지 src 가져와서 이미지 full url을 만듭니다.\n",
    "            image_url = \"https://pixabay.com\" + image.attrs['src']\n",
    "            filename = image_url.split('/')[-1]\n",
    "            \n",
    "            print image_url, filename\n",
    "            \n",
    "            # 로컬 컴퓨터에 이미지 다운로드 받기\n",
    "            download_image(image_url, filename)   \n",
    "\n",
    "crawl_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imdb에서 영화 정보 크롤링해보기\n",
    "\n",
    "타겟 URL: http://www.imdb.com/movies-coming-soon/2016-01/\n",
    "\n",
    "이곳에 들어가면 영화 제목, 장르, 평점, 감독, 배우 등의 정보가 있습니다. URL도 2015-01, 2015-02.. 이런식으로 바꿀 수 있습니다. 우선 2016-01 영화 정보를 함께 크롤링해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015-01 ~ 2015-12까지 순회하며 크롤링을 할 것이므로, url 파라미터를 받는 함수를 만듭니다.\n",
    "def movie_crawler(url):\n",
    "    res = requests.get(url)\n",
    "\n",
    "    # 예상치 못하게 404나 500을 받았는데, html을 파싱하려고 하면\n",
    "    # 에러가 발생하므로, 200(성공)일때만 코드가 실행되도록 합니다.\n",
    "    if res.status_code == 200:\n",
    "        soup = bs(res.text)\n",
    "\n",
    "        #\n",
    "        # titles = soup.select('#main > div > div.list.detail > div > table > tbody > tr > td.overview-top > h4 > a')\n",
    "        # images = soup.select('#img_primary > div > a > div > img')\n",
    "        #\n",
    "        # 이렇게 전체 html에서 titles(제목들), images(포스터 이미지들)를 가져오는 대신\n",
    "        # 하나의 movie 정보를 감싸고 있는 div 태그를 가져온 다음\n",
    "        # 그 안에 있는 포스터 이미지, 영화 제목, 평점 등을 가져오면\n",
    "        # for문을 한번만 돌아도됩니다!\n",
    "\n",
    "        # 이렇게 css selector를 가지고 받아올 수도 있고\n",
    "        movies = soup.select('#main > div > div.list.detail > div')\n",
    "        # 이렇게 속성을 가지고 받아올 수도 있습니다.\n",
    "        movies = soup.findAll('div', {'itemtype': 'http://schema.org/Movie'})\n",
    "\n",
    "        # title, image, running_time, score, genre, directors, actors 순으로 넣겠습니다.\n",
    "        table = []\n",
    "        for movie in movies:\n",
    "            row = []\n",
    "\n",
    "            title = movie.findAll('h4', {'itemprop': 'name'})\n",
    "            image = movie.findAll('img', {'itemprop': 'image'})\n",
    "            running_time = movie.findAll('time', {'itemprop': 'duration'})\n",
    "            score = movie.select('div.rating_txt > div > strong')\n",
    "            genres = movie.findAll('span', {'itemprop': 'genre'})\n",
    "            directors = movie.findAll('span', {'itemprop': 'director'})\n",
    "            actors = movie.findAll('span', {'itemprop': 'actors'})\n",
    "\n",
    "\n",
    "            # strip()은 앞뒤 공백을 지워줍니다.\n",
    "            # title가 빈 리스트([])인데, title[0]를 하면 index out of range 에러가 납니다.\n",
    "            # 에러가 나는 것을 방지해주기 위해서 뒤에 len(title)이 0보다 클때만 title[0]를 하게 하고\n",
    "            # 아니면 \"\" 빈 스트링을 row에 append 합니다.\n",
    "            row.append(title[0].text.strip() if len(title)>0 else \"\")\n",
    "            row.append(image[0]['src'].strip() if len(image)>0 else \"\")\n",
    "            row.append(running_time[0].text.strip() if len(running_time)>0 else \"\")\n",
    "            row.append(score[0].text.strip() if len(score)>0 else \"\")\n",
    "            \n",
    "            # genre, director, actor는 여러명인 경우가 있기 때문에 list로 받아옵니다.\n",
    "            row.append([genre.text.strip() for genre in genres])\n",
    "            row.append([director.text.strip() for director in directors])\n",
    "            row.append([actor.text.strip() for actor in actors])\n",
    "\n",
    "            # 예상대로라면\n",
    "            # row <= [u'The Woman in Black 2: Angel of Death (2014)', 'http://ia.media-imdb.com/images/M/MV5BMTgxMjUyNTAxNF5BMl5BanBnXkFtZTgwNTk4MDUyMzE@._V1_UY209_CR0,0,140,209_AL_.jpg', u'98 min', u'42', [u'Drama', u'Horror', u'Thriller'], [u'Tom Harper'], [u'Helen McCrory', u'Jeremy Irvine', u'Phoebe Fox', u'Leanne Best']]\n",
    "            # print row\n",
    "    \n",
    "            table.append(row)\n",
    "\n",
    "        return table\n",
    "\n",
    "jan_2015 = movie_crawler('http://www.imdb.com/movies-coming-soon/2015-01')\n",
    "# print jan_2015[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.imdb.com/movies-coming-soon/2015-01 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-02 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-03 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-04 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-05 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-06 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-07 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-08 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-09 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-10 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-11 crawling..\n",
      "http://www.imdb.com/movies-coming-soon/2015-12 crawling..\n"
     ]
    }
   ],
   "source": [
    "# 그러면 이제 2015-01부터 2015-12까지 영화정보를 긁어봅시다.\n",
    "target_url = 'http://www.imdb.com/movies-coming-soon/{0}'\n",
    "movie_total = []\n",
    "for i in range(1,13):\n",
    "    # string.zfill(2)을 사용해보세요. zero padding이 생깁니다.\n",
    "    date = \"2015-\" + str(i).zfill(2)\n",
    "    print target_url.format(date) + \" crawling..\"\n",
    "    movie_total += movie_crawler(target_url.format(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총389개의 영화 정보를 크롤링했습니다.\n",
      "========================================\n",
      "[u'The Woman in Black 2: Angel of Death (2014)', 'http://ia.media-imdb.com/images/M/MV5BMTgxMjUyNTAxNF5BMl5BanBnXkFtZTgwNTk4MDUyMzE@._V1_UY209_CR0,0,140,209_AL_.jpg', u'98 min', u'42', [u'Drama', u'Horror', u'Thriller'], [u'Tom Harper'], [u'Helen McCrory', u'Jeremy Irvine', u'Phoebe Fox', u'Leanne Best']]\n",
      "========================================\n",
      "[u'A Most Violent Year (2014)', 'http://ia.media-imdb.com/images/M/MV5BMjE4OTY4ODg3Ml5BMl5BanBnXkFtZTgwMTI1MTg1MzE@._V1_UY209_CR0,0,140,209_AL_.jpg', u'125 min', u'79', [u'Action', u'Crime', u'Drama', u'Thriller'], [u'J.C. Chandor'], [u'Oscar Isaac', u'Jessica Chastain', u'David Oyelowo', u'Alessandro Nivola']]\n",
      "========================================\n",
      "[u'Leviafan (2014)', 'http://ia.media-imdb.com/images/M/MV5BMjAwMTY3MTU0Ml5BMl5BanBnXkFtZTgwNzE0ODAwMzE@._V1_UY209_CR0,0,140,209_AL_.jpg', u'140 min', u'92', [u'Drama'], [u'Andrey Zvyagintsev'], [u'Aleksey Serebryakov', u'Elena Lyadova', u'Roman Madyanov', u'Vladimir Vdovichenkov']]\n"
     ]
    }
   ],
   "source": [
    "print \"총\" + str(len(movie_total)) + \"개의 영화 정보를 크롤링했습니다.\"\n",
    "\n",
    "# 3개만 출력해봅시다.\n",
    "for i in range(0,3):\n",
    "    print \"========================================\"\n",
    "    print movie_total[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적인 결과물은 아래와 같이 2차원 배열 형태입니다.\n",
    "\n",
    "\n",
    "```python\n",
    "movie_total = [\n",
    " ['영화 제목1', '포스터 이미지1', '...'],\n",
    " ['영화 제목2', '포스터 이미지2', '...'],\n",
    " ['영화 제목3', '포스터 이미지3', '...'],\n",
    " '...'\n",
    "]\n",
    "```\n",
    "\n",
    "다음 시간에는 이 데이터를 csv 파일로 export하고, Pandas, Numpy 라이브러리 활용법에 대해 알아보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
